2014-01-18
==========
So far I've been able to plot the data on a 2D space. But now that I
need to figure how to plot the decision boundary.

Will need to split the test data so that I can use the cross-validation
set to determine the optimal value for the regularization parameter.


2014-01-20
==========
We need to make sure that we also find the proper C value for regularization


2014-01-21
==========
Trying to figure out how to create a gridsearch of all possible gamma
and C parameters for svm. Ran gridsearch:

The best classifier is:  SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,
  gamma=0.01, kernel=rbf, max_iter=-1, probability=False,
  random_state=None, shrinking=True, tol=0.001, verbose=False)

2014-01-29
==========
Performed a grid search for the optimal C and gamma parameters for the
RBF kernel. This is done on the new dataset that was agreed upon with
Bhargav and Victor, where the training data is split to 60-20-20, where
the numbers correspond to how much (%) of the training set will be split
to be the training, cross-validation, and test set. 

I got C = 0.01, gamma = 0.01.

Result:
# of errors: 25, Percent Error: 12.50%
# of correct: 175, Percent Corr: 87.50%

Applied PCA on the dataset. Much of the components have comparable
amount of explained variance. About the first 35 components hold 98% of
the explained variance. 
    
From this point on, the training set (which is 60% of the original
training data) will be referred to as just the training set/data.

After applying PCA on the training data, used that transform to
transform the cross-validating set and the test set. Then performed
another grid-search for the best C and gamma parameters and obtained the
same set of values/parameters for the SVM RBF kernel. 

The result was a 12% error, a .5% improvement from the earlier one. 

Result:
# of errors: 24, Percent Error: 12.00%
# of correct: 176, Percent Corr: 88.00%

I don't think PCA offers much. 
